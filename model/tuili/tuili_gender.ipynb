{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: gensim in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from gensim) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.24)\n",
      "Requirement already satisfied: boto in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.24 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.24)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.24->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.24->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting keras==2.1.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/86/45/a273fe3f8fe931a11da34fba1cb74013cfc70dcf93e5d8d329c951dc44c5/Keras-2.1.4-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[K     |████████████████████████████████| 322 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (1.18.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (5.3.1)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "Successfully installed keras-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install keras==2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#推理代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/ipykernel/__main__.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4445721, 256)\n",
      "(3812203, 256)\n",
      "(62966, 128)\n",
      "(44316, 128)\n",
      "(337, 64)\n",
      "(4445721, 192)\n",
      "(3812203, 192)\n",
      "(62966, 128)\n",
      "(44316, 128)\n",
      "(337, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function  # do not delete this line if you want to save your log file.\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import *\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import gensim\n",
    "from numpy import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.set_random_seed(1024)\n",
    "\n",
    "# 得到emb矩阵\n",
    "emb1 = np.load('emb/embcreative_id_256.npy')\n",
    "print(emb1.shape)\n",
    "emb2 = np.load('emb/embad_id_256.npy')\n",
    "print(emb2.shape)\n",
    "emb3 = np.load('emb/embadvertiser_id_128.npy')\n",
    "print(emb3.shape)\n",
    "emb4 = np.load('emb/embproduct_id_128.npy')\n",
    "print(emb4.shape)\n",
    "emb5 = np.load('emb/embindustry_64.npy')\n",
    "print(emb5.shape)\n",
    "gc.collect()\n",
    "emb6 = np.load('DeepWalk/emb_creative_id.npy')\n",
    "print(emb6.shape)\n",
    "emb7 = np.load('DeepWalk/emb_ad_id.npy')\n",
    "print(emb7.shape)\n",
    "emb8 = np.load('DeepWalk/emb_advertiser_id.npy')\n",
    "print(emb8.shape)\n",
    "emb9 = np.load('DeepWalk/emb_product_id.npy')\n",
    "print(emb9.shape)\n",
    "emb10 = np.load('DeepWalk/emb_industry.npy')\n",
    "print(emb10.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要用到的函数\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        #super(AdamW, self).__init__(**kwargs)\n",
    "        super().__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # decoupled weight decay (2/4)\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay')\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd  # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            # decoupled weight decay (4/4)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv(emb1, emb2,emb3,emb4,emb5,emb6,emb7,emb8,emb9,emb10,num_feature_input):\n",
    "    '''\n",
    "    注意这个inputs\n",
    "    seq1、seq2分别是两个输入\n",
    "    hin是feature层输入\n",
    "    是否做emb可选可不选，\n",
    "    这个就是我们之前训练已经得到的用于embedding的（embedding_matrix1， embedding_matrix2）\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_2 = Embedding(\n",
    "        input_dim=emb2.shape[0],\n",
    "        output_dim=emb2.shape[1],\n",
    "        weights=[emb2],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_4 = Embedding(\n",
    "        input_dim=emb4.shape[0],\n",
    "        output_dim=emb4.shape[1],\n",
    "        weights=[emb4],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_5 = Embedding(\n",
    "        input_dim=emb5.shape[0],\n",
    "        output_dim=emb5.shape[1],\n",
    "        weights=[emb5],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_6 = Embedding(\n",
    "        input_dim=emb6.shape[0],\n",
    "        output_dim=emb6.shape[1],\n",
    "        weights=[emb6],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_7 = Embedding(\n",
    "        input_dim=emb7.shape[0],\n",
    "        output_dim=emb7.shape[1],\n",
    "        weights=[emb7],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_8 = Embedding(\n",
    "        input_dim=emb8.shape[0],\n",
    "        output_dim=emb8.shape[1],\n",
    "        weights=[emb8],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_9 = Embedding(\n",
    "        input_dim=emb9.shape[0],\n",
    "        output_dim=emb9.shape[1],\n",
    "        weights=[emb9],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_10 = Embedding(\n",
    "        input_dim=emb10.shape[0],\n",
    "        output_dim=emb10.shape[1],\n",
    "        weights=[emb10],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    seq1 = Input(shape=(100,))\n",
    "    seq2 = Input(shape=(100,))\n",
    "    seq3 = Input(shape=(100,))\n",
    "    seq4 = Input(shape=(100,))\n",
    "    seq5 = Input(shape=(100,))\n",
    "    \n",
    "\n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x2 = emb_layer_2(seq2)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    x4 = emb_layer_4(seq4)\n",
    "    x5 = emb_layer_5(seq5)\n",
    "    \n",
    "    x6 = emb_layer_6(seq1)\n",
    "    x7 = emb_layer_7(seq2)\n",
    "    x8 = emb_layer_8(seq3)\n",
    "    x9 = emb_layer_9(seq4)\n",
    "    x10 = emb_layer_10(seq5)\n",
    "    \n",
    "    \n",
    "    sdrop = SpatialDropout1D(rate=0.2) #某个区域全部置0，就是某一列全部置0\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x2 = sdrop(x2)\n",
    "    x3 = sdrop(x3)\n",
    "    x4 = sdrop(x4)\n",
    "    x5 = sdrop(x5)\n",
    "    \n",
    "    x6 = sdrop(x6)\n",
    "    x7 = sdrop(x7)\n",
    "    x8 = sdrop(x8)\n",
    "    x9 = sdrop(x9)\n",
    "    x10 = sdrop(x10)\n",
    "    \n",
    "    x11 = Concatenate()([x1,x2,x3,x5,x4,x6,x7,x8,x10,x9])  \n",
    "    #x12 = Concatenate()([x6,x7,x8,x10,x9]) \n",
    "    \n",
    "    input_mask = Input(shape=(1,))\n",
    "    \n",
    "    x_11 = Dropout(0.35)(Bidirectional(CuDNNLSTM(256, return_sequences=True))(x11))#双向LSTM\n",
    "    x_11 = TimeDistributed(Dense(256, activation=\"relu\"))(x_11)\n",
    "    semantic = TimeDistributed(Dense(128, activation=\"relu\"))(x_11)\n",
    "    merged_1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(128,))(semantic)\n",
    "    merged_1_avg = Lambda(lambda x: K.sum(x, axis=1)/input_mask, output_shape=(128,))(semantic)\n",
    "    \n",
    "    \"\"\"\n",
    "    x_12 = Dropout(0.35)(Bidirectional(CuDNNLSTM(256, return_sequences=True))(x12))#双向LSTM\n",
    "    x_12 = TimeDistributed(Dense(256, activation=\"relu\"))(x_12)\n",
    "    semantic = TimeDistributed(Dense(128, activation=\"relu\"))(x_12)\n",
    "    merged_2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(128,))(semantic)\n",
    "    merged_2_avg = Lambda(lambda x: K.sum(x, axis=1)/input_mask, output_shape=(128,))(semantic)\n",
    "    \"\"\"\n",
    "    \n",
    "    stat_in = Input(shape=(num_feature_input,))\n",
    "    stat_fea = Dense(128, activation='relu')(stat_in)\n",
    "    \n",
    "    x = Concatenate()([merged_1,merged_1_avg,stat_fea]) \n",
    "    \n",
    "    x = Dropout(0.4)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(x))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    pred = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=[seq1,seq2,seq3,seq4,seq5,input_mask,stat_in], outputs=pred)\n",
    "    from keras.utils import multi_gpu_model\n",
    "    #model = multi_gpu_model(model, 2)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=AdamW(lr=3e-4, weight_decay=0.08, ), metrics=[\"accuracy\"])# AdamW(lr=0.001, weight_decay=0.08, ), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:88: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:91: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:166: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:171: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:176: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:180: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:189: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:196: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3135: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/optimizers.py:752: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_age = model_conv(emb1,emb2,emb3,emb4,emb5,emb6,emb7,emb8,emb9,emb10,252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_generator(inputPath, tfidfinputPath,countinputPath,targetPath,all_num , batch_size, mode=\"train\",turn=\"false\"):\n",
    "    f = h5py.File(inputPath, \"r\")\n",
    "    f2 = h5py.File(tfidfinputPath, \"r\")\n",
    "    f3 = h5py.File(countinputPath, \"r\")\n",
    "    f4 = h5py.File(targetPath,\"r\")\n",
    "    all_sum = all_num/batch_size\n",
    "    flag = 0-batch_size\n",
    "    beta = 0.35\n",
    "    alpha = 0.2\n",
    "    #augu =1\n",
    "    while True:\n",
    "        \n",
    "        flag += batch_size\n",
    "        if(flag==all_num):\n",
    "            flag=0  #切换下一个epoch\n",
    "            #augu+=1 #3代后进行数据增强\n",
    "        labels = []\n",
    "        input1 = f['cre'][flag:flag+batch_size] #输入w2v的序列\n",
    "        input2 = f['ad'][flag:flag+batch_size]\n",
    "        input3 = f['adv'][flag:flag+batch_size]\n",
    "        input4 = f['pro'][flag:flag+batch_size]\n",
    "        input5 = f['ind'][flag:flag+batch_size]\n",
    "        \n",
    "        input6 = f2['tfidf'][flag:flag+batch_size] #输入tfidf\n",
    "        input7 = f3['count'][flag:flag+batch_size] #输入count\n",
    "        \n",
    "        input8 = f4['target'][flag:flag+batch_size] #输入target_encoding\n",
    "        \n",
    "        input9 = np.hstack((input6,input8))\n",
    "        \n",
    "        input_mask  = np.zeros(batch_size)\n",
    "        if mode=='train':\n",
    "            lens = 0\n",
    "            #if augu>=3:\n",
    "            for i in range(batch_size):\n",
    "                input_mask[i]=lens = len(np.nonzero(input1[i])[0])\n",
    "                num = np.random.rand()\n",
    "                if num > beta:\n",
    "                    state = np.random.get_state()\n",
    "                    np.random.shuffle(input1[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input2[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input3[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input4[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input5[i][100-lens:100])\n",
    "                if num > alpha:\n",
    "                    input1[i][100-lens:100] = input1[i][100-lens:100][::-1] \n",
    "                    input2[i][100-lens:100] = input2[i][100-lens:100][::-1] \n",
    "                    input3[i][100-lens:100] = input3[i][100-lens:100][::-1] \n",
    "                    input4[i][100-lens:100] = input4[i][100-lens:100][::-1] \n",
    "                    input5[i][100-lens:100] = input5[i][100-lens:100][::-1] \n",
    "                    continue\n",
    "            indexs = f['gender'][flag:flag+batch_size]\n",
    "            labels = to_categorical(indexs, num_classes=2) #one-hot\n",
    "                    \n",
    "            #labels = label_smoothing(labels) #smoothing\n",
    "        if mode=='vail':\n",
    "            indexs = f['gender'][flag:flag+batch_size]\n",
    "            labels = to_categorical(indexs, num_classes=2) #one-hot\n",
    "        \n",
    "        if mode=='vail' or mode==\"test\":\n",
    "            for i in range(batch_size):\n",
    "                input_mask[i]= len(np.nonzero(input1[i])[0])\n",
    "            if turn==\"true\":  #翻转\n",
    "                for i in range(batch_size):\n",
    "                    lens = len(np.nonzero(input1[i])[0])\n",
    "                    input1[i][100-lens:100] = input1[i][100-lens:100][::-1] \n",
    "                    input2[i][100-lens:100] = input2[i][100-lens:100][::-1] \n",
    "                    input3[i][100-lens:100] = input3[i][100-lens:100][::-1] \n",
    "                    input4[i][100-lens:100] = input4[i][100-lens:100][::-1] \n",
    "                    input5[i][100-lens:100] = input5[i][100-lens:100][::-1] \n",
    "                    continue\n",
    "                \n",
    "        \n",
    "        yield ([input1, input2,input3,input4,input5,input_mask,input6], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 54s 453ms/step\n",
      "120/120 [==============================] - 46s 386ms/step\n",
      "200/200 [==============================] - 77s 386ms/step\n",
      "200/200 [==============================] - 76s 380ms/step\n",
      "0.949085\n"
     ]
    }
   ],
   "source": [
    "i=4\n",
    "ceshi_out = i\n",
    "ps = i\n",
    "filepath = f\"./nn_pseudo_gender_model/nn_v1_{ceshi_out}.h5\"\n",
    "filepath_tr = f\"./w2v_feature/nn_feature{ps}/train{ps}.h5\"\n",
    "filepath_te = f\"./w2v_feature/nn_feature{ps}/vail{ps}.h5\"\n",
    "\n",
    "filepath_tr_tf = f\"./tfidf/nn_feature{ps}/train{ps}.h5\"\n",
    "filepath_te_tf = f\"./tfidf/nn_feature{ps}/vail{ps}.h5\"\n",
    "\n",
    "filepath_tr_co = f\"./count/nn_feature{ps}/train{ps}.h5\"\n",
    "filepath_te_co = f\"./count/nn_feature{ps}/vail{ps}.h5\"\n",
    "\n",
    "filepath_tr_ta = f\"./target_out/nn_feature{ps}/train{ps}.h5\"\n",
    "filepath_te_ta = f\"./target_out/nn_feature{ps}/vail{ps}.h5\"\n",
    "model_age.load_weights(filepath)\n",
    "\n",
    "train_batch = 1000 #1024\n",
    "vail_batch = 5000\n",
    "test_batch = 5000\n",
    "\n",
    "trainGen = h5_generator(inputPath=filepath_tr,tfidfinputPath=filepath_tr_tf,countinputPath=filepath_tr_co ,targetPath =filepath_tr_ta,all_num = 2400000, batch_size=train_batch, mode=\"train\")\n",
    "vailGen = h5_generator(inputPath=filepath_te,tfidfinputPath=filepath_te_tf,countinputPath=filepath_te_co ,targetPath=filepath_te_ta,all_num=600000, batch_size=vail_batch, mode=\"vail\",turn=\"false\")\n",
    "testGen = h5_generator(inputPath='w2v_feature/test.h5',tfidfinputPath='tfidf/test.h5', countinputPath='count/test.h5',targetPath='target_out/test.h5',all_num=1000000, batch_size=test_batch, mode=\"test\",turn=\"false\")\n",
    "\n",
    "vailGen_turn = h5_generator(inputPath=filepath_te,tfidfinputPath=filepath_te_tf,countinputPath=filepath_te_co ,targetPath=filepath_te_ta,all_num=600000, batch_size=vail_batch, mode=\"vail\",turn=\"true\")\n",
    "testGen_turn = h5_generator(inputPath='w2v_feature/test.h5',tfidfinputPath='tfidf/test.h5', countinputPath='count/test.h5',targetPath='target_out/test.h5',all_num=1000000, batch_size=test_batch, mode=\"test\",turn=\"true\")\n",
    "\n",
    "\n",
    "steps_epoch = 2400000//train_batch\n",
    "vail_steps_epoch = 600000//vail_batch\n",
    "test_epoch = 1000000//test_batch\n",
    "\n",
    "vailGen_score = model_age.predict_generator(vailGen, verbose=1,steps=vail_steps_epoch)\n",
    "vailGen_score_turn = model_age.predict_generator(vailGen_turn, verbose=1,steps=vail_steps_epoch)\n",
    "sub = model_age.predict_generator(testGen, verbose=1,steps=200)\n",
    "sub_turn = model_age.predict_generator(testGen_turn, verbose=1,steps=200)\n",
    "\n",
    "submit = (sub +sub_turn)/2\n",
    "np.save(f'./result_final4_model_answer/tuili_gender/test{ceshi_out}.npy',submit)\n",
    "np.save(f'./result_final4_model_answer/tuili_gender_one/test{ceshi_out}.npy',sub)\n",
    "train = (vailGen_score+vailGen_score_turn)/2\n",
    "np.save(f'./result_final4_model_answer/tuili_gender/train{ceshi_out}.npy',train)\n",
    "np.save(f'./result_final4_model_answer/tuili_gender_one/train{ceshi_out}.npy',vailGen_score)\n",
    "\n",
    "f = h5py.File(filepath_te, \"r\")\n",
    "indexs = f['gender']\n",
    "\n",
    "score = accuracy_score(f['gender'][:],argmax(vailGen_score,axis=1))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting gensim\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from gensim) (1.5.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0b/8e/464b06f5efd26f2dc16ce7bd1662c2f31cadf9104fdbcbf5994674cc3a51/smart_open-2.1.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Collecting boto\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/07/d5/c77db61800f7134c6f90aed2d27564542bf4b01b8f8f8dcbb0eb68316442/boto3-1.14.24-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.18.0,>=1.17.24\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/33/fb/01a9a52cb32704b46bfb81415f6ce892329d03df56f094fe14d923fa5fbb/botocore-1.17.24-py2.py3-none-any.whl (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.24->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110317 sha256=6960e7bf8cd7fd977007d314390f421f8bdfd842bf7fcc23e3331e2813cac202\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/af/b2/aa/49b1424dd5099959003661d365fe8dcec7c3ddf2290a91a568\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.14.24 botocore-1.17.24 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting keras==2.1.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/86/45/a273fe3f8fe931a11da34fba1cb74013cfc70dcf93e5d8d329c951dc44c5/Keras-2.1.4-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[K     |████████████████████████████████| 322 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (1.5.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.1.4) (5.3.1)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.4\n"
     ]
    }
   ],
   "source": [
    "#推理代码，需要对验证集和测试集进行翻转的增广操作\n",
    "#得更换网络结构和数据生成器代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install keras==2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 21 01:40:54 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   36C    P0    38W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   33C    P0    34W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:0A.0 Off |                    0 |\n",
      "| N/A   37C    P0    38W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   35C    P0    39W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#推理代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/ipykernel/__main__.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4445721, 256)\n",
      "(3812203, 256)\n",
      "(62966, 128)\n",
      "(44316, 128)\n",
      "(337, 64)\n",
      "(4445721, 192)\n",
      "(3812203, 192)\n",
      "(62966, 128)\n",
      "(44316, 128)\n",
      "(337, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function  # do not delete this line if you want to save your log file.\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import *\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import gensim\n",
    "from numpy import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.set_random_seed(1024)\n",
    "\n",
    "# 得到emb矩阵\n",
    "emb1 = np.load('emb/embcreative_id_256.npy')\n",
    "print(emb1.shape)\n",
    "emb2 = np.load('emb/embad_id_256.npy')\n",
    "print(emb2.shape)\n",
    "emb3 = np.load('emb/embadvertiser_id_128.npy')\n",
    "print(emb3.shape)\n",
    "emb4 = np.load('emb/embproduct_id_128.npy')\n",
    "print(emb4.shape)\n",
    "emb5 = np.load('emb/embindustry_64.npy')\n",
    "print(emb5.shape)\n",
    "gc.collect()\n",
    "emb6 = np.load('DeepWalk/emb_creative_id.npy')\n",
    "print(emb6.shape)\n",
    "emb7 = np.load('DeepWalk/emb_ad_id.npy')\n",
    "print(emb7.shape)\n",
    "emb8 = np.load('DeepWalk/emb_advertiser_id.npy')\n",
    "print(emb8.shape)\n",
    "emb9 = np.load('DeepWalk/emb_product_id.npy')\n",
    "print(emb9.shape)\n",
    "emb10 = np.load('DeepWalk/emb_industry.npy')\n",
    "print(emb10.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要用到的函数\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        #super(AdamW, self).__init__(**kwargs)\n",
    "        super().__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # decoupled weight decay (2/4)\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay')\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd  # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            # decoupled weight decay (4/4)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "def label_smoothing(inputs, epsilon=0.5):\n",
    "    K = inputs.shape[1]    # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)\n",
    "\n",
    "def model_conv(emb1, emb2,emb3,emb4,emb5,emb6,emb7,emb8,emb9,emb10,num_feature_input):\n",
    "    '''\n",
    "    注意这个inputs\n",
    "    seq1、seq2分别是两个输入\n",
    "    hin是feature层输入\n",
    "    是否做emb可选可不选，\n",
    "    这个就是我们之前训练已经得到的用于embedding的（embedding_matrix1， embedding_matrix2）\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_2 = Embedding(\n",
    "        input_dim=emb2.shape[0],\n",
    "        output_dim=emb2.shape[1],\n",
    "        weights=[emb2],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_4 = Embedding(\n",
    "        input_dim=emb4.shape[0],\n",
    "        output_dim=emb4.shape[1],\n",
    "        weights=[emb4],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_5 = Embedding(\n",
    "        input_dim=emb5.shape[0],\n",
    "        output_dim=emb5.shape[1],\n",
    "        weights=[emb5],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_6 = Embedding(\n",
    "        input_dim=emb6.shape[0],\n",
    "        output_dim=emb6.shape[1],\n",
    "        weights=[emb6],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_7 = Embedding(\n",
    "        input_dim=emb7.shape[0],\n",
    "        output_dim=emb7.shape[1],\n",
    "        weights=[emb7],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_8 = Embedding(\n",
    "        input_dim=emb8.shape[0],\n",
    "        output_dim=emb8.shape[1],\n",
    "        weights=[emb8],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    emb_layer_9 = Embedding(\n",
    "        input_dim=emb9.shape[0],\n",
    "        output_dim=emb9.shape[1],\n",
    "        weights=[emb9],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_10 = Embedding(\n",
    "        input_dim=emb10.shape[0],\n",
    "        output_dim=emb10.shape[1],\n",
    "        weights=[emb10],\n",
    "        input_length=100,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    seq1 = Input(shape=(100,))\n",
    "    seq2 = Input(shape=(100,))\n",
    "    seq3 = Input(shape=(100,))\n",
    "    seq4 = Input(shape=(100,))\n",
    "    seq5 = Input(shape=(100,))\n",
    "    \n",
    "\n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x2 = emb_layer_2(seq2)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    x4 = emb_layer_4(seq4)\n",
    "    x5 = emb_layer_5(seq5)\n",
    "    \n",
    "    x6 = emb_layer_6(seq1)\n",
    "    x7 = emb_layer_7(seq2)\n",
    "    x8 = emb_layer_8(seq3)\n",
    "    x9 = emb_layer_9(seq4)\n",
    "    x10 = emb_layer_10(seq5)\n",
    "    \n",
    "    \n",
    "    sdrop = SpatialDropout1D(rate=0.2) #某个区域全部置0，就是某一列全部置0\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x2 = sdrop(x2)\n",
    "    x3 = sdrop(x3)\n",
    "    x4 = sdrop(x4)\n",
    "    x5 = sdrop(x5)\n",
    "    \n",
    "    x6 = sdrop(x6)\n",
    "    x7 = sdrop(x7)\n",
    "    x8 = sdrop(x8)\n",
    "    x9 = sdrop(x9)\n",
    "    x10 = sdrop(x10)\n",
    "    \n",
    "    x11 = Concatenate()([x1,x2,x3,x5,x4,x6,x7,x8,x10,x9])  \n",
    "    #x12 = Concatenate()([x6,x7,x8,x10,x9]) \n",
    "    \n",
    "    input_mask = Input(shape=(1,))\n",
    "    \n",
    "    x_11 = Dropout(0.35)(Bidirectional(CuDNNLSTM(256, return_sequences=True))(x11))#双向LSTM\n",
    "    x_11 = TimeDistributed(Dense(256, activation=\"relu\"))(x_11)\n",
    "    semantic = TimeDistributed(Dense(128, activation=\"relu\"))(x_11)\n",
    "    merged_1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(128,))(semantic)\n",
    "    merged_1_avg = Lambda(lambda x: K.sum(x, axis=1)/input_mask, output_shape=(128,))(semantic)\n",
    "    \n",
    "    \"\"\"\n",
    "    x_12 = Dropout(0.35)(Bidirectional(CuDNNLSTM(256, return_sequences=True))(x12))#双向LSTM\n",
    "    x_12 = TimeDistributed(Dense(256, activation=\"relu\"))(x_12)\n",
    "    semantic = TimeDistributed(Dense(128, activation=\"relu\"))(x_12)\n",
    "    merged_2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(128,))(semantic)\n",
    "    merged_2_avg = Lambda(lambda x: K.sum(x, axis=1)/input_mask, output_shape=(128,))(semantic)\n",
    "    \"\"\"\n",
    "    \n",
    "    stat_in = Input(shape=(num_feature_input,))\n",
    "    stat_fea = Dense(128, activation='relu')(stat_in)\n",
    "    \n",
    "    x = Concatenate()([merged_1,merged_1_avg,stat_fea]) \n",
    "    \n",
    "    x = Dropout(0.4)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(x))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    pred = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs=[seq1,seq2,seq3,seq4,seq5,input_mask,stat_in], outputs=pred)\n",
    "    from keras.utils import multi_gpu_model\n",
    "    #model = multi_gpu_model(model, 2)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=AdamW(lr=0.001, weight_decay=0.08, ), metrics=[\"accuracy\"])# AdamW(lr=0.001, weight_decay=0.08, ), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:88: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:91: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:166: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:171: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:176: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:180: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:189: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:196: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3135: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/optimizers.py:752: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_age = model_conv(emb1,emb2,emb3,emb4,emb5,emb6,emb7,emb8,emb9,emb10,252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_generator(inputPath, tfidfinputPath,countinputPath,targetPath,all_num , batch_size, mode=\"train\",turn=\"false\"):\n",
    "    f = h5py.File(inputPath, \"r\")\n",
    "    f2 = h5py.File(tfidfinputPath, \"r\")\n",
    "    f3 = h5py.File(countinputPath, \"r\")\n",
    "    f4 = h5py.File(targetPath,\"r\")\n",
    "    all_sum = all_num/batch_size\n",
    "    flag = 0-batch_size\n",
    "    beta = 0.35\n",
    "    alpha = 0.2\n",
    "    ff = open(f'./pseudo_label/pseudo_label{ps}.csv', \"r\")#0.5258的分数\n",
    "    ff.readline()\n",
    "    for i in range(ps*600000): ff.readline()#跳到对应折的标签\n",
    "    #augu =1\n",
    "    while True:\n",
    "        \n",
    "        \n",
    "        flag += batch_size\n",
    "        if(flag==all_num):\n",
    "            flag=0  #切换下一个epoch\n",
    "            #augu+=1 #3代后进行数据增强\n",
    "        labels = []\n",
    "        input1 = f['cre'][flag:flag+batch_size] #输入w2v的序列\n",
    "        input2 = f['ad'][flag:flag+batch_size]\n",
    "        input3 = f['adv'][flag:flag+batch_size]\n",
    "        input4 = f['pro'][flag:flag+batch_size]\n",
    "        input5 = f['ind'][flag:flag+batch_size]\n",
    "        \n",
    "        input6 = f2['tfidf'][flag:flag+batch_size] #输入tfidf\n",
    "        input7 = f3['count'][flag:flag+batch_size] #输入count\n",
    "        \n",
    "        input8 = f4['target'][flag:flag+batch_size] #输入target_encoding\n",
    "        \n",
    "        input9 = np.hstack((input6,input8))\n",
    "        \n",
    "        input_mask  = np.zeros(batch_size)\n",
    "        if mode=='train':\n",
    "            lens = 0\n",
    "            #if augu>=3:\n",
    "            for i in range(batch_size):\n",
    "                input_mask[i]=lens = len(np.nonzero(input1[i])[0])\n",
    "                num = np.random.rand()\n",
    "                if num > beta:\n",
    "                    state = np.random.get_state()\n",
    "                    np.random.shuffle(input1[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input2[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input3[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input4[i][100-lens:100])\n",
    "                    np.random.set_state(state)\n",
    "                    np.random.shuffle(input5[i][100-lens:100])\n",
    "                if num > alpha:\n",
    "                    input1[i][100-lens:100] = input1[i][100-lens:100][::-1] \n",
    "                    input2[i][100-lens:100] = input2[i][100-lens:100][::-1] \n",
    "                    input3[i][100-lens:100] = input3[i][100-lens:100][::-1] \n",
    "                    input4[i][100-lens:100] = input4[i][100-lens:100][::-1] \n",
    "                    input5[i][100-lens:100] = input5[i][100-lens:100][::-1] \n",
    "                    continue\n",
    "            \n",
    "            pseudolabels = []\n",
    "            while len(pseudolabels) < batch_size:\n",
    "                line = ff.readline()\n",
    "                if line == \"\":\n",
    "                    ff.seek(0)\n",
    "                    ff.readline()\n",
    "                    line = ff.readline()\n",
    "                line = line.strip().split(\",\")\n",
    "                pseudolabel = np.array([float(x) for x in line])\n",
    "\n",
    "                pseudolabels.append(pseudolabel)\n",
    "    \n",
    "            indexs = f['age'][flag:flag+batch_size]\n",
    "            labels = to_categorical(indexs, num_classes=10) #one-hot\n",
    "            labels = 0.7*labels + 0.3*np.array(pseudolabels)\n",
    "                    \n",
    "            #labels = label_smoothing(labels) #smoothing\n",
    "        if mode=='vail':\n",
    "            indexs = f['age'][flag:flag+batch_size]\n",
    "            labels = to_categorical(indexs, num_classes=10) #one-hot\n",
    "        \n",
    "        if mode=='vail' or mode==\"test\":\n",
    "            for i in range(batch_size):\n",
    "                input_mask[i]= len(np.nonzero(input1[i])[0])\n",
    "            if turn==\"true\":  #翻转\n",
    "                for i in range(batch_size):\n",
    "                    lens = len(np.nonzero(input1[i])[0])\n",
    "                    input1[i][100-lens:100] = input1[i][100-lens:100][::-1] \n",
    "                    input2[i][100-lens:100] = input2[i][100-lens:100][::-1] \n",
    "                    input3[i][100-lens:100] = input3[i][100-lens:100][::-1] \n",
    "                    input4[i][100-lens:100] = input4[i][100-lens:100][::-1] \n",
    "                    input5[i][100-lens:100] = input5[i][100-lens:100][::-1] \n",
    "                    continue\n",
    "                \n",
    "        \n",
    "        yield ([input1, input2,input3,input4,input5,input_mask,input6], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 57s 476ms/step\n",
      "120/120 [==============================] - 46s 384ms/step\n",
      "200/200 [==============================] - 82s 412ms/step\n",
      "200/200 [==============================] - 76s 379ms/step\n",
      "0.520595\n",
      "120/120 [==============================] - 53s 445ms/step\n",
      "120/120 [==============================] - 47s 389ms/step\n",
      "200/200 [==============================] - 76s 380ms/step\n",
      "200/200 [==============================] - 76s 382ms/step\n",
      "0.520725\n",
      "120/120 [==============================] - 56s 464ms/step\n",
      "120/120 [==============================] - 47s 391ms/step\n",
      "200/200 [==============================] - 76s 382ms/step\n",
      "200/200 [==============================] - 77s 383ms/step\n",
      "0.5214083333333334\n",
      "120/120 [==============================] - 56s 467ms/step\n",
      "120/120 [==============================] - 47s 395ms/step\n",
      "200/200 [==============================] - 77s 386ms/step\n",
      "200/200 [==============================] - 77s 383ms/step\n",
      "0.5204083333333334\n",
      "120/120 [==============================] - 59s 488ms/step\n",
      "120/120 [==============================] - 48s 400ms/step\n",
      "200/200 [==============================] - 77s 386ms/step\n",
      "200/200 [==============================] - 77s 387ms/step\n",
      "0.5206283333333334\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    ceshi_out = i\n",
    "    ps = i\n",
    "    filepath = f\"./nn_pseudo_model/nn_v1_{ceshi_out}.h5\"\n",
    "    filepath_tr = f\"./w2v_feature/nn_feature{ps}/train{ps}.h5\"\n",
    "    filepath_te = f\"./w2v_feature/nn_feature{ps}/vail{ps}.h5\"\n",
    "\n",
    "    filepath_tr_tf = f\"./tfidf/nn_feature{ps}/train{ps}.h5\"\n",
    "    filepath_te_tf = f\"./tfidf/nn_feature{ps}/vail{ps}.h5\"\n",
    "\n",
    "    filepath_tr_co = f\"./count/nn_feature{ps}/train{ps}.h5\"\n",
    "    filepath_te_co = f\"./count/nn_feature{ps}/vail{ps}.h5\"\n",
    "    \n",
    "    filepath_tr_ta = f\"./target_out/nn_feature{ps}/train{ps}.h5\"\n",
    "    filepath_te_ta = f\"./target_out/nn_feature{ps}/vail{ps}.h5\"\n",
    "\n",
    "    model_age.load_weights(filepath)\n",
    "    \n",
    "    train_batch = 1000 #1024\n",
    "    vail_batch = 5000\n",
    "    test_batch = 5000\n",
    "    trainGen = h5_generator(inputPath=filepath_tr,tfidfinputPath=filepath_tr_tf,countinputPath=filepath_tr_co ,targetPath =filepath_tr_ta, all_num = 2400000, batch_size=train_batch, mode=\"train\")\n",
    "    vailGen = h5_generator(inputPath=filepath_te,tfidfinputPath=filepath_te_tf,countinputPath=filepath_te_co ,targetPath=filepath_te_ta, all_num=600000, batch_size=vail_batch, mode=\"vail\",turn=\"false\")\n",
    "    testGen = h5_generator(inputPath='w2v_feature/test.h5',tfidfinputPath='tfidf/test.h5', countinputPath='count/test.h5',targetPath='target_out/test.h5',all_num=1000000, batch_size=test_batch, mode=\"test\",turn=\"false\")\n",
    "\n",
    "\n",
    "    vailGen_turn = h5_generator(inputPath=filepath_te,tfidfinputPath=filepath_te_tf,countinputPath=filepath_te_co ,targetPath=filepath_te_ta,all_num=600000, batch_size=vail_batch, mode=\"vail\",turn=\"true\")\n",
    "    testGen_turn = h5_generator(inputPath='w2v_feature/test.h5',tfidfinputPath='tfidf/test.h5', countinputPath='count/test.h5',targetPath='target_out/test.h5',all_num=1000000, batch_size=test_batch, mode=\"test\",turn=\"true\")\n",
    "\n",
    "\n",
    "    steps_epoch = 2400000//train_batch\n",
    "    vail_steps_epoch = 600000//vail_batch\n",
    "    test_epoch = 1000000//test_batch\n",
    "\n",
    "    vailGen_score = model_age.predict_generator(vailGen, verbose=1,steps=vail_steps_epoch)\n",
    "    vailGen_score_turn = model_age.predict_generator(vailGen_turn, verbose=1,steps=vail_steps_epoch)\n",
    "    sub = model_age.predict_generator(testGen, verbose=1,steps=200)\n",
    "    sub_turn = model_age.predict_generator(testGen_turn, verbose=1,steps=200)\n",
    "\n",
    "    submit = (sub +sub_turn)/2\n",
    "    np.save(f'./result_final4_model_answer_pseudo/tuili/test{ceshi_out}.npy',submit)\n",
    "    np.save(f'./result_final4_model_answer_pseudo/tuili_one/test{ceshi_out}.npy',sub)\n",
    "    train = (vailGen_score+vailGen_score_turn)/2\n",
    "    np.save(f'./result_final4_model_answer_pseudo/tuili/train{ceshi_out}.npy',train)\n",
    "    np.save(f'./result_final4_model_answer_pseudo/tuili_one/train{ceshi_out}.npy',vailGen_score)\n",
    "\n",
    "    f = h5py.File(filepath_te, \"r\")\n",
    "    indexs = f['age']\n",
    "\n",
    "    score = accuracy_score(f['age'][:],argmax(vailGen_score,axis=1))\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "120/120 [==============================] - 55s 462ms/step\n",
    "120/120 [==============================] - 55s 462ms/step\n",
    "200/200 [==============================] - 92s 458ms/step\n",
    "200/200 [==============================] - 92s 459ms/step\n",
    "0.5166866666666666\n",
    "120/120 [==============================] - 55s 460ms/step\n",
    "120/120 [==============================] - 55s 462ms/step\n",
    "200/200 [==============================] - 92s 458ms/step\n",
    "200/200 [==============================] - 92s 461ms/step\n",
    "0.5160016666666667\n",
    "120/120 [==============================] - 55s 461ms/step\n",
    "120/120 [==============================] - 56s 464ms/step\n",
    "200/200 [==============================] - 92s 461ms/step\n",
    "200/200 [==============================] - 93s 463ms/step\n",
    "0.5163366666666667\n",
    "120/120 [==============================] - 56s 463ms/step\n",
    "120/120 [==============================] - 56s 463ms/step\n",
    "200/200 [==============================] - 92s 460ms/step\n",
    "200/200 [==============================] - 93s 463ms/step\n",
    "0.515065\n",
    "120/120 [==============================] - 55s 460ms/step\n",
    "120/120 [==============================] - 56s 464ms/step\n",
    "200/200 [==============================] - 92s 461ms/step\n",
    "200/200 [==============================] - 93s 463ms/step\n",
    "0.5157433333333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fold_5(dir,name):\n",
    "    train0 = np.load(dir+'train0.npy')\n",
    "    train1 = np.load(dir+'train1.npy')\n",
    "    train2 = np.load(dir+'train2.npy')\n",
    "    train3 = np.load(dir+'train3.npy')\n",
    "    train4 = np.load(dir+'train4.npy')\n",
    "\n",
    "    test0 = np.load(dir+'test0.npy')\n",
    "    test1 = np.load(dir+'test1.npy')\n",
    "    test2 = np.load(dir+'test2.npy')\n",
    "    test3 = np.load(dir+'test3.npy')\n",
    "    test4 = np.load(dir+'test4.npy')\n",
    "\n",
    "    train_all = np.vstack((train0,train1,train2,train3,train4))\n",
    "    test_all = (test0+test1+test2+test3+test4)/5\n",
    "\n",
    "    label = pd.read_pickle(\"label.pkl\")\n",
    "    score = accuracy_score(label[name],argmax(train_all,axis=1)+1)\n",
    "    print(score)\n",
    "    \n",
    "    np.save(dir + \"train_all.npy\",train_all)\n",
    "    np.save(dir + \"test_all.npy\",test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuili_one_dir =\"./result_final4_model_answer_pseudo/tuili_one/\"\n",
    "tuili_dir = \"./result_final4_model_answer_pseudo/tuili/\"\n",
    "\n",
    "tuili_gender_one_dir =\"./result_final4_model_answer/tuili_gender_one/\"\n",
    "tuili_gender_dir = \"./result_final4_model_answer/tuili_gender/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = tuili_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.520753\n",
      "0.5211703333333333\n"
     ]
    }
   ],
   "source": [
    "Fold_5(tuili_one_dir,name='age')\n",
    "Fold_5(tuili_dir,name='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9491403333333334\n",
      "0.9491693333333333\n"
     ]
    }
   ],
   "source": [
    "Fold_5(tuili_gender_one_dir,name='gender')\n",
    "Fold_5(tuili_gender_dir,name='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
